# ğŸ” Application de Recherche Documentaire RAG (DockerisÃ©e)

Application de recherche documentaire utilisant une architecture RAG (Retrieval-Augmented Generation) pour interroger un corpus de documents avec des rÃ©ponses gÃ©nÃ©rÃ©es par IA. **EntiÃ¨rement conteneurisÃ©e avec Docker** pour une reproductibilitÃ© maximale.

## ğŸ¯ FonctionnalitÃ©s

- **Indexation automatique** des documents (PDF, TXT, MD, DOCX)
- **Recherche sÃ©mantique** avec embeddings Sentence Transformers
- **GÃ©nÃ©ration de rÃ©ponses** contextualisÃ©es avec Ollama/Mistral
- **Interface utilisateur** moderne avec Streamlit
- **Stockage vectoriel persistant** avec ChromaDB
- **Affichage des sources** utilisÃ©es pour chaque rÃ©ponse
- **DÃ©ploiement Docker** en un seul clic

## ğŸ—ï¸ Architecture Technique

- **LlamaIndex**: Orchestration RAG et gestion du pipeline
- **ChromaDB**: Base de donnÃ©es vectorielle pour le stockage des embeddings
- **Sentence Transformers**: ModÃ¨le d'embeddings (`all-MiniLM-L6-v2`)
- **Ollama/Mistral**: ModÃ¨le de langage pour la gÃ©nÃ©ration de rÃ©ponses
- **Streamlit**: Interface utilisateur web
- **Docker**: Conteneurisation et orchestration

## ğŸ“‹ PrÃ©requis

**Uniquement Docker et Docker Compose !**

```bash
# VÃ©rifier Docker
docker --version

# VÃ©rifier Docker Compose
docker-compose --version
```

**Configuration minimale requise :**
- 8 GB RAM
- 10 GB d'espace disque libre
- Connexion Internet (pour tÃ©lÃ©charger Mistral au premier dÃ©marrage)

## ğŸš€ DÃ©marrage Rapide

### 1. PrÃ©parer vos documents
Placez vos documents dans le rÃ©pertoire `./data`:
```bash
cp /chemin/vers/vos/documents/*.pdf ./data/
```

Formats supportÃ©s: `.pdf`, `.txt`, `.md`, `.docx`

### 2. Lancer l'application
```bash
# Construire et dÃ©marrer tous les services
docker-compose up -d

# Suivre les logs (optionnel)
docker-compose logs -f
```

> **Note :** Au premier dÃ©marrage, le modÃ¨le Mistral (~4GB) sera tÃ©lÃ©chargÃ© automatiquement. Cela peut prendre 2-5 minutes selon votre connexion.

### 3. AccÃ©der Ã  l'application
Ouvrez votre navigateur Ã  l'adresse : **http://localhost:8501**

### 4. Utiliser l'application

1. **PremiÃ¨re utilisation**: L'indexation se fera automatiquement au premier lancement
2. **Poser une question**: Entrez votre question dans le champ de texte
3. **Consulter les rÃ©sultats**: La rÃ©ponse s'affichera avec les sources utilisÃ©es
4. **Ajouter des documents**: 
   - Ajoutez de nouveaux fichiers dans `./data`
   - Cliquez sur "ğŸ”„ RÃ©indexer les documents" dans la sidebar

## ğŸ“ Structure du Projet

```
octoscrub/
â”œâ”€â”€ app.py                  # Application Streamlit principale
â”œâ”€â”€ config.py               # Configuration centralisÃ©e
â”œâ”€â”€ document_indexer.py     # Module d'indexation des documents
â”œâ”€â”€ query_engine.py         # Module de traitement des requÃªtes
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ Dockerfile              # Image Docker de l'application
â”œâ”€â”€ docker-compose.yml      # Orchestration des services
â”œâ”€â”€ .dockerignore           # Exclusions Docker
â”œâ”€â”€ .gitignore              # Exclusions Git
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ data/                   # Documents Ã  indexer (volume Docker)
â””â”€â”€ chroma_db/              # Base vectorielle (volume Docker)
```

## ğŸ³ Gestion Docker

### Commandes utiles

```bash
# DÃ©marrer les services
docker-compose up -d

# ArrÃªter les services
docker-compose down

# Voir les logs
docker-compose logs -f app
docker-compose logs -f ollama

# RedÃ©marrer un service
docker-compose restart app

# Reconstruire l'image
docker-compose build --no-cache

# Voir l'Ã©tat des services
docker-compose ps

# Nettoyer tout (ATTENTION : supprime les volumes !)
docker-compose down -v
```

### Services

L'application utilise 2 services Docker :

1. **ollama** : Serveur Ollama avec le modÃ¨le Mistral
   - Port interne : 11434
   - Volume : `ollama_data` (modÃ¨les persistants)

2. **app** : Application Streamlit avec RAG
   - Port : 8501
   - Volumes : `./data`, `./chroma_db`

## ğŸ”§ Configuration

### Variables d'environnement

Vous pouvez personnaliser la configuration dans `docker-compose.yml` :

```yaml
environment:
  - OLLAMA_BASE_URL=http://ollama:11434
```

### ParamÃ¨tres de l'application

Modifiez `config.py` pour ajuster :

```python
# ModÃ¨le d'embeddings
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# ModÃ¨le LLM
OLLAMA_MODEL = "mistral"

# Nombre de sources Ã  rÃ©cupÃ©rer
SIMILARITY_TOP_K = 5

# Taille des chunks de texte
CHUNK_SIZE = 1024
CHUNK_OVERLAP = 200
```

AprÃ¨s modification, redÃ©marrez :
```bash
docker-compose restart app
```

## ğŸ› DÃ©pannage

### Le service Ollama ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose logs ollama

# RedÃ©marrer le service
docker-compose restart ollama
```

### L'application ne trouve pas Ollama
VÃ©rifiez que les 2 services sont UP :
```bash
docker-compose ps
```

Les deux doivent afficher "Up" et "healthy".

### Erreur de mÃ©moire
Augmentez la RAM allouÃ©e Ã  Docker dans les paramÃ¨tres Docker Desktop (minimum 8GB recommandÃ©).

### Le modÃ¨le Mistral n'est pas tÃ©lÃ©chargÃ©
```bash
# TÃ©lÃ©charger manuellement
docker-compose exec ollama ollama pull mistral

# VÃ©rifier les modÃ¨les installÃ©s
docker-compose exec ollama ollama list
```

### Nettoyer et redÃ©marrer
```bash
# ArrÃªter tout
docker-compose down

# Supprimer les volumes (ATTENTION : perd les donnÃ©es)
docker volume rm octoscrub_ollama_data

# RedÃ©marrer
docker-compose up -d
```

## ğŸ“ Exemples de Questions

- "De quoi parlent les documents ?"
- "Quelles sont les informations principales ?"
- "RÃ©sume le contenu des documents"
- "Explique [concept spÃ©cifique] mentionnÃ© dans les documents"

## ğŸ”’ ConfidentialitÃ©

- Tous les traitements sont effectuÃ©s **localement** dans vos conteneurs Docker
- Aucune donnÃ©e n'est envoyÃ©e Ã  des services externes
- Les embeddings et l'index sont stockÃ©s localement dans `./chroma_db`
- Le modÃ¨le Mistral tourne localement via Ollama

## ğŸ“Š Volumes et Persistance

Les donnÃ©es suivantes sont persistantes entre les redÃ©marrages :

- **./data** : Vos documents (montÃ© depuis l'hÃ´te)
- **./chroma_db** : Base vectorielle ChromaDB (montÃ© depuis l'hÃ´te)
- **ollama_data** : ModÃ¨les Ollama (volume Docker nommÃ©)

Pour sauvegarder vos donnÃ©es :
```bash
# Sauvegarde de la base vectorielle
tar -czf chroma_db_backup.tar.gz chroma_db/

# Restauration
tar -xzf chroma_db_backup.tar.gz
```

## ğŸš¢ DÃ©ploiement en Production

### Avec port mapping personnalisÃ©
```yaml
# Dans docker-compose.yml
services:
  app:
    ports:
      - "8080:8501"  # AccÃ¨s via http://localhost:8080
```

### DerriÃ¨re un reverse proxy (nginx/traefik)
L'application Ã©coute sur `0.0.0.0:8501` et est prÃªte pour un reverse proxy.

## ğŸ“„ Licence

Ce projet est fourni Ã  des fins Ã©ducatives et de dÃ©monstration.

## ğŸ¤ Contribution

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue.

---

**DÃ©veloppÃ© avec â¤ï¸ en utilisant LlamaIndex, ChromaDB, Sentence Transformers, Ollama/Mistral, Streamlit et Docker**
